---
description: 기획서를 다관점 서브 에이전트로 평가합니다
argument-hint: <파일경로> <에이전트수(2~7)>
command-version: 2
---

기획서를 다관점 서브 에이전트로 평가합니다. 사용법: /LaLaAdvice <파일경로> <에이전트수(2~7)>

## 인자 파싱

`$ARGUMENTS`는 `<파일경로> <에이전트수>` 형식입니다.
- 에이전트 수가 생략되면 사용자에게 몇 명의 서브 에이전트로 평가할지 질문하세요. (추천: 3~5명)
- 에이전트 수는 2 이상 7 이하여야 합니다. 범위를 벗어나면 안내 후 재입력을 요청하세요.

예시: `/LaLaAdvice docs/기획서.md 3`

## 워크플로우

### 1단계: 파일 읽기 및 섹션 파싱

1. 지정된 마크다운 파일을 읽습니다.
2. 최상위 섹션을 파싱합니다. 섹션은 `## 1. 제목`, `## 2. 제목` 등 번호가 붙은 헤딩 또는 최상위 `##` 헤딩 단위로 구분합니다.
3. 파싱된 섹션 목록을 사용자에게 보여줍니다.

### 2단계: 다관점 서브 에이전트 평가

아래 관점 풀에서 에이전트 수만큼 선택하여 **Task 도구로 서브 에이전트를 병렬 실행**합니다.
반드시 **한 번의 메시지에서 모든 Task 호출을 동시에** 보내세요.

**관점 풀** (에이전트 수에 맞춰 위에서부터 순서대로 선택):
1. **사용자 관점 (User Perspective)**: 최종 사용자의 입장에서 기획서를 평가합니다. 사용성, 사용자 경험, 접근성, 사용자 시나리오의 현실성을 중심으로 분석합니다.
2. **기술 관점 (Technical Perspective)**: 개발자의 입장에서 평가합니다. 구현 가능성, 기술적 모호함, 성능 고려사항, 아키텍처 타당성을 중심으로 분석합니다.
3. **비즈니스 관점 (Business Perspective)**: 비즈니스 의사결정자의 입장에서 평가합니다. 시장 가치, ROI, 경쟁력, 수익 모델, 확장성을 중심으로 분석합니다.
4. **품질 관점 (Quality Perspective)**: QA 엔지니어의 입장에서 평가합니다. 테스트 가능성, 예외 상황, 엣지 케이스, 검증 기준의 명확성을 중심으로 분석합니다.
5. **보안·규정 관점 (Security & Compliance Perspective)**: 보안 전문가의 입장에서 평가합니다. 보안 위협, 개인정보 보호, 법적 규정 준수, 데이터 관리 정책을 중심으로 분석합니다.
6. **디자인 관점 (Design Perspective)**: UX/UI 디자이너의 입장에서 평가합니다. 정보 구조, 인터랙션 흐름, 시각적 일관성, 사용자 피드백 설계를 중심으로 분석합니다.
7. **운영 관점 (Operations Perspective)**: 서비스 운영자의 입장에서 평가합니다. 모니터링, 장애 대응, 배포 전략, 운영 비용, 확장 가능한 인프라를 중심으로 분석합니다.

**각 서브 에이전트에게 전달할 프롬프트 템플릿:**

```
당신은 [관점명]을 가진 기획서 평가 전문가입니다.

아래 기획서를 읽고, [관점 설명]을 중심으로 각 섹션을 평가하세요.

**중요: 이 작업은 읽기 전용 리서치입니다. 절대로 파일을 생성하거나 수정하지 마세요. 분석 결과를 텍스트로만 반환하세요.**

## 기획서 파일
[파일경로]

## 평가 기준
- 해당 관점에서 각 섹션의 강점과 약점을 분석합니다.
- 구체적이고 실행 가능한 개선점을 제시합니다.
- 심각도를 표시합니다: 🔴 심각 / 🟡 주의 / 🟢 양호

## 출력 형식
각 섹션에 대해 아래 형식으로 출력하세요:

### 섹션: [섹션 제목]
- **평가**: [관점에서의 전체 평가 1~2문장]
- **강점**: [잘 된 점]
- **개선점**:
  - [심각도] [구체적인 문제와 개선 제안]
- **점수**: [1~10점] / 10
```

### 3단계: 통합 평가

모든 서브 에이전트의 결과를 수집한 뒤, **직접** 통합 평가를 수행합니다.

통합 평가에서는 다음을 검증합니다:
1. **섹션 간 논리적 일관성**: 서로 다른 섹션에서 모순되는 내용이 없는지 확인
2. **용어 일관성**: 동일한 개념에 대해 다른 용어를 사용하고 있지 않은지 확인
3. **흐름 연결성**: 섹션 순서가 논리적이며, 앞 섹션의 결론이 뒷 섹션의 전제와 맞는지 확인
4. **누락 검증**: 기획서에 있어야 할 필수 섹션이 빠져 있지 않은지 확인
5. **중복 검증**: 불필요하게 반복되는 내용이 없는지 확인
6. **실행 가능성 종합**: 전체적으로 이 기획서대로 실행했을 때 현실적인지 종합 판단
7. **다관점 교차 분석**: 서브 에이전트들의 평가가 상충하는 부분을 식별하고 중재

### 4단계: 최종 보고서 생성

`.alyplan/<baseName>/` 폴더 안에 `<baseName>.advice.md` 파일을 생성합니다.
- 예: `docs/example.md` → `.alyplan/example/example.advice.md`
- `.alyplan/<baseName>/` 폴더가 없으면 생성합니다.

**보고서 형식:**

```markdown
# 기획서 다관점 평가 보고서

> 원본: [파일명] | 평가 에이전트: [N]명 | [날짜]

---

## 종합 점수

| 관점 | 점수 | 핵심 의견 |
|------|------|-----------|
| [관점1] | X/10 | [한 줄 요약] |
| [관점2] | X/10 | [한 줄 요약] |
| ... | ... | ... |
| **종합** | **X/10** | **[종합 한 줄 요약]** |

---

## 섹션별 다관점 평가

### 섹션 1: [제목]

#### [관점1 이름]
[해당 관점의 평가 내용]

#### [관점2 이름]
[해당 관점의 평가 내용]

...

#### 섹션 종합
[이 섹션에 대한 종합 의견]

---

(나머지 섹션 반복)

---

## 통합 검증

### 섹션 간 논리적 일관성
[검증 결과]

### 용어 일관성
[검증 결과]

### 흐름 연결성
[검증 결과]

### 누락 검증
[검증 결과]

### 중복 검증
[검증 결과]

### 실행 가능성 종합
[검증 결과]

### 다관점 교차 분석
[서브 에이전트 간 의견이 상충하는 부분과 중재 의견]

---

## 핵심 권고사항

1. [심각] [가장 중요한 개선 사항]
2. [주의] [두 번째 개선 사항]
3. ...

```

## 규칙

- 서브 에이전트는 반드시 Task 도구의 `subagent_type: "general-purpose"`로 실행합니다.
- 서브 에이전트들은 반드시 **병렬로** 실행합니다 (한 번의 메시지에 모든 Task 호출).
- 서브 에이전트에게는 **파일을 읽고 분석만** 하라고 명시하세요. **절대로** 파일을 생성하거나 수정하지 않습니다. 분석 결과를 텍스트로만 반환합니다.
- 통합 평가는 메인 에이전트(당신)가 직접 수행합니다. 서브 에이전트에게 위임하지 마세요.
- 보고서는 한국어로 작성합니다.
- 각 섹션의 점수는 1~10점 스케일을 사용합니다.
- 최종 보고서 파일명은 반드시 `<baseName>.advice.md`입니다. (`advice-review.md`가 아닙니다)
- 최종 보고서 생성 후 사용자에게 파일 경로와 핵심 발견사항 요약을 알려주세요.
